---
title: "Percentile Demonstration"
author: "D.E. Beaudette"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
    keep_md: no
---

```{r setup, echo=FALSE, results='hide', warning=FALSE}
# setup
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', dev='svglite', tidy=FALSE, verbose=FALSE, cache = FALSE)
options(width=100, stringsAsFactors=FALSE, cache=FALSE)
```


```{r  echo=FALSE, results='hide', warning=FALSE}
library(aqp)
library(soilDB)
library(scales)
library(Hmisc)
library(rgdal)
library(raster)
library(e1071)
library(sharpshootR)

## TODO: there are better ways to do this now

# get a subset of a given variable from an SPC, by pattern matching against hz designation
subsetData <- function(SPC, hzname, hz_pat, var) {
  
  h <- horizons(SPC)
  idx <- grep(hz_pat, h[[hzname]])
  x <- h[[var]][idx]
  return(x)
}


# toggle for re-making data cache
reMakeData <- FALSE

if(reMakeData) {
  # get some sample data
  kssl <- fetchKSSL('miami')
  nasis <- fetchNASIS(rmHzErrors = FALSE)
  
  idx <- grep('loafercreek', nasis$taxonname, ignore.case = TRUE)
  loafercreek <- nasis[idx, ]
  
  idx <- grep('amador', nasis$taxonname, ignore.case = TRUE)
  amador <- nasis[idx, ]
  
  idx <- grep('Nedsgulch', nasis$taxonname, ignore.case = TRUE)
  nedsgulch <- nasis[idx, ]
  
  # best possible scenario: rasters are in memory
  r.elev <- readAll(raster('E:/gis_data/ca630/ca630_elev/hdr.adf'))
  r.slope <- readAll(raster('E:/gis_data/ca630/ca630_slope/hdr.adf'))
  r.MAP <- readAll(raster('E:/gis_data/prism/final_MAP_mm_800m.tif'))
  
  # load map unit polygons
  mu <-  readOGR(dsn='E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb', layer='ca630_a', stringsAsFactors = FALSE)
  
  # add a unique polygon ID
  mu$pID <- seq(from=1, to=length(mu))
  
  # extract polygons for a single map unit ("MUSYM" attribute = "7089")
  # note that column names in your data may be different
  mu.5012 <- mu[which(mu$MUSYM == '5012'), ]
  
  # generate sampling points
  s <- constantDensitySampling(mu.5012, n.pts.per.ac=1, min.samples=1, polygon.id='pID')
  
  # extract raster data
  e.elev <- extract(r.elev, s)
  e.slope <- extract(r.slope, s)
  e.MAP <- extract(r.MAP, s)
  
  # save for later
  save(kssl, loafercreek, amador, nedsgulch, e.elev, e.slope, e.MAP, file='cached-data.rda')
} else {
  # get cached version
  load(file='cached-data.rda')
}

```

```{r echo=FALSE}
# example data for introduction
set.seed(101010)
x <- round(rnorm(11, mean=15, sd=4))
```

# Introduction
This document describes the *percentile* as a robust measure of central tendency and spread within a distribution of values. Examples are given for its application in the context of soil data summaries.

## Definition and Description of the Percentile
Within a set of data, the *n-th* [percentile](https://en.wikipedia.org/wiki/Percentile) describes the value below which *n%* of the data, when sorted, fall. For example, within the integer sequence spanning from 0 to 100, *50* is the 50th percentile or median, *10* is the 10th percentile, and *90* is the 90th percentile.

Consider the following (hypothetical) field-described clay content from the A horizon of the same taxa:

`r x`

sorted:

`r sort(x)`

resulting: 
 
 * 10th percentile: **`r quantile(x, probs=c(0.1))`**
 * 50th percentile: **`r quantile(x, probs=c(0.5))`**
 * 90th percentile: **`r quantile(x, probs=c(0.9))`**



### Visual Demonstration
Consider a histogram derived from carbon stock values representing various regions of the US. Within this *distribution* a carbon stock of 85 tons/ha is associated with the 16th percentile. In other words, 16% of the collected carbon stock values are less than 85 tons/ha.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
# 1000 normally-distributed random values, mean = 100, SD = 15
set.seed(1010101)
x <- rnorm(1000, 100, 15)
# create empirical cumulative distribution function
e <- ecdf(x)
# pick a single point
p <- 85
# annotate
p.lab <- sprintf('%s tons/ha\n%sth percentile', round(p), round(e(p) * 100))

# plot
par(mar=c(6,1,4,1))
hist(x, axes=FALSE, xlab='', ylab='', main='Carbon Stock: All Regions', breaks=50, col = grey(0.9), border = grey(0.85))
axis(1, at=pretty(x, n = 10), cex.axis=0.75)
points(p, 0, pch=21, bg='RoyalBlue', col='black', cex=2)
axis(1, at=p, labels = p.lab, cex.axis=0.75, line=2.5, lwd=2, tcl=2, col = 'RoyalBlue', font=2)
box()
```



## Why Percentiles?

* Percentiles require no distributional assumptions and are bound to the data from which they are computed. This means that percentiles can provide meaningful benchmarks for both normal and non-normal distributions, and, the limits will always fall within the min/max of the observed data.

* Direct interpretation; consider the 10th ($P_{10}$) and 90th ($P_{90}$) percentiles: "given the available data, we know that soil property $p < P_{10}$ 10% of the time, and, $p < P_{90}$ 90% of the time". This same statement can be framed using probabilities or proportions: "given the available data, soil property $p$ is within the range of $\{P_{10} - P_{90}\}$ 80% of the time".

* Percentiles are simple to calculate, requiring at least 3, better 10, and ideally $>$ 20 observations.

* The median is a robust estimator of [central tendency](https://en.wikipedia.org/wiki/Central_tendency).

* The lower and upper percentiles (e.g. 10th and 90th) a robust estimator of spread.

* Statistics such as the mean, standard deviation, and confidence intervals are based on the normal distribution.


## Small Sample Sizes and Interpolation
Estimation of percentiles is based on ranking of the original data. Interpolation *between observed values* is required when sample size is small (generally less than 10 observations). Consider the values `(1,3,5,6,7,9,9,10)`. Estimation of the 10th, 50th, and 90th percentiles results in `r round(Hmisc::hdquantile(c(1,3,5,6,7,9,9,10), probs = c(0.1, 0.5, 0.9)))` respectively. Since we are not typically interested in the estimated percentiles verbatim, the interpolated estimates are close enough. The [Harrel-Davis estimator](https://rdrr.io/cran/Hmisc/man/hdquantile.html) is a robust method for deriving percentiles in the presence of ties and when sample size is small.


# Common Distribution Shapes

The following figures demonstrate the relationship between distribution shape, measures of central tendency (mean and 50th percentile), and measures of spread (mean +/- 2 standard deviations, and 10th / 90th percentiles). Within each figure is an *idealized* normal distribution that is based on the sample mean and standard deviation. The y-axis can be interpreted as the "relative proportion" of samples associated with a value on the x-axis. The thick, smooth lines represent an estimate of [density](https://en.wikipedia.org/wiki/Density_estimation) using real measurements, a continuous alternative to the [histogram](https://en.wikipedia.org/wiki/Histogram) (grey columns).


## Symmetry Around the Central Tendency
With a large enough sample size, the distribution of some soil properties can be approximated with the [normal or Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution). In this case, the mean and median are practically equal and the spread around the central tendency is symmetric. Examples include lab measured clay content or pH, from a collection of related samples (e.g. A horizons from a single soil series concept).

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(3,1,2,1))
set.seed(101010)
x <- rnorm(100, mean=15, sd=2)
res <- percentileDemo(x, xlab='Hypothetical Values', ylab='', main='Approximately Normal')
```

## Skewness: Asymmetric "Tails"
Various forms of the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) are typically more accurate approximations of soil properties. Log-normal distributions with a "short tail", or a low degree of asymmetry around the central tendency ([skewness](https://en.wikipedia.org/wiki/Skewness)), are common. Note the shift between mean and median, and the unequal distances to 10th and 90th percentiles. Examples include lab measured organic carbon and field measured rock fragment volume.
```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(3,1,2,1))
set.seed(101010)
x <- rlnorm(100, meanlog = log(15), sdlog = log(1.5))
res <- percentileDemo(x, xlab='Hypothetical Values', ylab='', main='Log-Normal: Short Tail')
kable(res, digits = 1)
```

Log-normal distributions with a "long tail", e.g. more [skewed](https://en.wikipedia.org/wiki/Skewness), are commonly encountered when summarizing GIS data sources such as elevation, slope, and curvature. Note that the mean +/- 2SD is no longer a meaningful representation of spread around the central tendency.
```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5}
par(mar=c(1,1,2,1))
set.seed(101010)
x <- rlnorm(100, meanlog = log(15), sdlog = log(3.8))
res <- percentileDemo(x, xlab='Hypothetical Values', ylab='', main='Log-Normal: Long Tail')
```

In general, the further the departure from a normal distribution, the less meaningful mean and standard deviation are as metrics of central tendency and spread.

## Caution: Comparison of Apples and Oranges
A mixture of incompatible data (e.g. A and Bt clay content) will always result in unreliable summary statistics. Consider the following hypothetical, [multimodal distribution](https://en.wikipedia.org/wiki/Multimodal_distribution) of clay content resulting from erroneously combining data from A and Bt horizons. In this case the estimates of central tendency are representative of neither group and the estimates of spread are misleading. A graphical inspection of distribution shape is critical to *meaningful* estimation of central tendency and spread.
```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
set.seed(101010)
x <- c(rnorm(100, mean=10, sd=0.8), rnorm(100, mean=20, sd=1.5))
res <- percentileDemo(x, xlab='', main='Hypothetical Bimodal Distribution', labels.signif = 1, hist.breaks = 60)
```

# Examples


## Lab Characterization Data
The following examples are based on KSSL data correlated to the [Miami](https://casoilresource.lawr.ucdavis.edu/sde/?series=miami) series.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Ap', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='KSSL: Miami, Ap Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Ap', var = 'estimated_oc')
res <- percentileDemo(x, xlab='Organic Carbon (%)', main='KSSL: Miami, Ap Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Bt1', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='KSSL: Miami, Bt1 Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Bt1', var = 'estimated_ph_h2o')
res <- percentileDemo(x, xlab='pH', main='KSSL: Miami, Bt1 Horizons', labels.signif = 2)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
kssl$thick <- (kssl$hzn_top + kssl$hzn_bot) / 2
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Bt1', var = 'thick')
res <- percentileDemo(x, xlab='Horizon Thickness (cm)', main='KSSL: Miami, Bt1 Horizons', labels.signif = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=8.5, fig.height=8.5, fig.keep='last'}
# probably not the most efficient
sand <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Ap', var = 'sand')
silt <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Ap', var = 'silt')
clay <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Ap', var = 'clay')

# prepare data into the format expected by textureTriangleSummary
ssc <- na.omit(
  data.frame(
    SAND = sand, 
    SILT = silt, 
    CLAY = clay)
)

textureTriangleSummary(ssc, cex = 0.75, p = c(0.1, 0.5, 0.9), main = 'KSSL: Miami, Ap Horizons')
```



## Morpologic Data (NASIS)

### Pedons Correlated to Loafercreek

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'Bt', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Loafercreek, Bt Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'A', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='NASIS: Loafercreek, A Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'Bt', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='NASIS: Loafercreek, Bt Horizons')
```


### Pedons Correlated to Nedsgulch

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'A', var = 'sand')
res <- percentileDemo(x, xlab='Percent Sand', main='NASIS: Nedsgulch, A Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'Bt', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Nedsgulch, Bt Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'A', var = 'phfield')
res <- percentileDemo(x, xlab='pH', main='NASIS: Nedsgulch, A Horizons', labels.signif = 2)
```


### Pedons Correlated to Amador
Note that histogram and density estimates are not very helpful when sample size is small. Also, note that estimated percentiles are interpolated between actual observations.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(amador, hzname = 'hzname', hz_pat = 'A', var = 'clay')
res <- percentileDemo(x, xlab='Percent Sand', main='NASIS: Amador, A Horizons')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(amador, hzname = 'hzname', hz_pat = 'A', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Amador, A Horizons')
```

## GIS Data
Many sampled values leads to more reliable estimates of central tendency and spread.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.elev, hist.breaks=90, xlab='Elevation (m)', main='CA630: Map Unit 5012')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.slope, hist.breaks=90, xlab='Slope (%)', main='CA630: Map Unit 5012')
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.MAP, hist.breaks=90, xlab='PRISM Mean Annual PPT (mm)', main='CA630: Map Unit 5012')
```

# Re-Create With Your Own Data

Install relevant packages from CRAN and the development version of `sharpshootR` from [GitHub](https://github.com/ncss-tech/sharpshootR).
```{r eval=FALSE}
install.packages('sharpshootR', dep=TRUE)
install.packages('e1071', dep=TRUE)
devtools::install_github("ncss-tech/sharpshootR", dependencies=FALSE, upgrade_dependencies=FALSE)
```

Grab some data and make your own figures.
```{r eval=FALSE}
library(soilDB)
library(sharpshootR)

# from pedons in your NASIS selected set
x <- fetchNASIS()
```

Create subset of relevant data:

 * pedons correlated to Nedsgulch
 * horizon designation matching `Bt1`
```{r eval=FALSE}
# filter taxonname
idx <- grep('Nedsgulch', x$taxonname, ignore.case = TRUE)
nedsgulch <- x[idx, ]
h <- horizons(nedsgulch)

# filter horizon designation
idx <- grep('Bt1', h$hzname)
z <- h$clay[idx]

# make figure
percentileDemo(z, hist.breaks=30, xlab='Field Described Percent Clay', main='NASIS: Nedsgulch, Bt1 horizons')
```

## Empirical Cumulative Distribution Function (ECDF)

```{r eval=FALSE}
# 1000 normally-distributed random values, mean = 100, SD = 15
x <- rnorm(1000, 100, 15)
# create empirical cumulative distribution function
e <- ecdf(x)
# pick a single point
p <- sample(x, 1)
# annotate
p.lab <- paste0(round(p), ' tons/ha\n', round(e(p) * 100), 'th percentile of all regions')

# plot
par(mar=c(6,1,4,1))
hist(x, axes=FALSE, xlab='', ylab='', main='Carbon Stocks of all Regions', breaks=50, col = grey(0.9), border = grey(0.85))
axis(1, at=pretty(x, n = 10), cex.axis=0.75)
points(p, 0, pch=21, bg='RoyalBlue', col='black', cex=2)
axis(1, at=p, labels = p.lab, cex.axis=0.75, line=2.5, lwd=2, tcl=2, col = 'RoyalBlue')
```


----------------------------------------------
This document is based on `aqp` version `r utils::packageDescription("aqp", field="Version")`, `soilDB` version `r utils::packageDescription("soilDB", field="Version")`, and `sharpshootR` version `r utils::packageDescription("sharpshootR", field="Version")`.





