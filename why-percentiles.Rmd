---
title: "A Case for Percentiles"
subtitle: ""
author: "D.E. Beaudette"
date: "`r Sys.Date()`"
output:
  tint::tintHtml: 
    self_contained: TRUE
    smart: yes
    keep_md: no
link-citations: yes
---

```{r setup, echo=FALSE, results='hide', warning=FALSE}
# setup
library(tint)
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, cache = FALSE)
options(width=100, stringsAsFactors=FALSE, cache=FALSE)
```


```{r  echo=FALSE, results='hide', warning=FALSE}
# convenient summary of vector x, can contain NA
f.summary <- function(x) {
  n <- length(na.omit(x))
  frac.NA <- round( (1 - ( length(na.omit(x)) / length(x) ) ) * 100)
  x.mean <- mean(x, na.rm=TRUE)
  x.sd <- sd(x, na.rm=TRUE)
  x.min <- min(x, na.rm=TRUE)
  x.max <- max(x, na.rm=TRUE)
  pr <- c(0.05, 0.1, 0.5, 0.9, 0.95)
  q <- t(quantile(x, probs = c(0.05, 0.1, 0.5, 0.9, 0.95), na.rm=TRUE))
  dimnames(q)[[2]] <- paste0('P', pr * 100)
  d <- data.frame(mean=x.mean, sd=x.sd, min=x.min, max=x.max, q, n, frac.NA)
  return(d)
}

## borrowed from MU reports
# remove NA
# re-scale to {0,1}
# return x,y values
scaled.density <- function(x) {
  res <- stats::density(na.omit(x), cut=2)
  return(data.frame(x=res$x, y=scales::rescale(res$y)))
}


compare.with.mean <- function(x) {
  x <- na.omit(x)
  s <- f.summary(x)
  
  # density plot of the data, scaled to {0,1}
  d.x <- scaled.density(x)
  # idealized normal over interval of the data
  x.seq <- seq(s$mean - 3 * s$sd, s$mean + 3 * s$sd, length.out = 100)
  ideal.normal <- cbind(x.seq, scales::rescale(dnorm(x.seq, mean=s$mean, sd=s$sd)))
  
  par(mar=c(3,1,1,1))
  plot(d.x, type='l', lwd=2, las=1, ylim=c(-0.15, 1.1), axes=FALSE)
  lines(ideal.normal)
  abline(h=0, col=grey(0.85))
  
  ## percentiles
  y.base <- -0.05
  # lower / upper pctile
  segments(x0=s$P10, x1=s$P90, y0=y.base, y1=y.base, col='RoyalBlue', lwd=2)
  segments(x0=c(s$P10, x1=s$P90), x1=c(s$P10, x1=s$P90), y0=y.base, y1=1, lty=3, col='RoyalBlue')
  # median
  segments(x0=s$P50, x1=s$P50, y0=y.base, y1=1, lty=3, col='RoyalBlue')
  points(s$P50, y.base, cex=1.2, pch=22, bg='RoyalBlue')
  
  ## idealized normal
  y.base <- -0.1
  # +/- SD
  segments(x0=(-1 * s$sd) + s$mean, x1=(1 * s$sd) + s$mean, y0=y.base, y1=y.base, col='Orange', lwd=2)
  segments(x0=(-1 * s$sd) + s$mean, x1=(-1 * s$sd) + s$mean, y0=y.base, y1=1, lty=3, col='Orange')
  segments(x0=(1 * s$sd) + s$mean, x1=(1 * s$sd) + s$mean, y0=y.base, y1=1, lty=3, col='Orange')
  # mean
  segments(x0=s$mean, x1=s$mean, y0=y.base, y1=1, lty=3, col='Orange')
  points(s$mean, y.base, cex=1.25, pch=22, bg='Orange')
  
  ## min / max
  y.base <- -0.15
  # +/- SD
  segments(x0=s$min, x1=s$max, y0=y.base, y1=y.base, col='Darkred', lwd=2)
  segments(x0=s$min, x1=s$min, y0=y.base, y1=1, lty=3, col='Darkred')
  segments(x0=s$max, x1=s$max, y0=y.base, y1=1, lty=3, col='Darkred')
  
  axis(side=1, at = pretty(x, n = 8))
  box()
  legend('top', lwd=2, lty=1, col=c('RoyalBlue', 'Orange', 'Darkred'), legend = c('Percentiles', 'Mean +/- SD', 'Min / Max'), horiz=TRUE, bty='n')
}

```



```{r echo=FALSE, results='hide', warning=FALSE}
library(soilDB)
library(scales)

# get some sample data
kssl <- fetchKSSL('plano')

var <- 'clay'
hz <- 'B'
hz.col <- 'hzn_desgn'

h <- horizons(kssl)
idx <- grep(hz, h[[hz.col]])
x <- h[[var]][idx]

compare.with.mean(x)

```

```{r}
# get some sample data
nasis <- fetchNASIS(rmHzErrors = FALSE)

idx <- grep('crimeahouse', nasis$taxonname, ignore.case = TRUE)
d <- nasis[idx, ]

var <- 'clay'
hz <- 'B'
hz.col <- 'hzname'

h <- horizons(nasis)
idx <- grep(hz, h[[hz.col]])
x <- h[[var]][idx]

compare.with.mean(x)
```



```{r}
library(rgdal)
library(raster)
library(sharpshootR)

# load map unit polygons
# Shapefile Example
mu <-  readOGR(dsn='E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb', layer='ca630_a', stringsAsFactors = FALSE)

# extract polygons for a single map unit ("MUSYM" attribute = "7089")
# note that column names in your data may be different
mu <- mu[which(mu$MUSYM == '7089'), ]

# add a unique polygon ID
mu$pID <- seq(from=1, to=length(mu))

# best possible scenario: rasters are in memory
r.elev <- readAll(raster('E:/gis_data/ca630/ca630_elev/hdr.adf'))
r.slope <- readAll(raster('E:/gis_data/ca630/ca630_slope/hdr.adf'))

s <- constantDensitySampling(mu, n.pts.per.ac=1, min.samples=1, polygon.id='pID')
```

# Introduction

Nearly all statistical methods are based on the premise that meaningful descriptions of an underlying population can be derived from a [subset of the population, or  *sample*](https://en.wikipedia.org/wiki/Sampling_(statistics)). The efficiency of a sample is (mostly) dependent on three parameters: the shape of the population distribution, the size of the sample and the degree to which values in the population are related ([autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)). A population with a simple distribution, such as the [normal](https://en.wikipedia.org/wiki/Normal_distribution), can be characterized with a smaller sample than more complex distributions. The effect of sample size makes intuitive sense: more observations (e.g. *larger* sample) will give a better description of the population, regardless of distribution. Characterization of populations with a low degree of autocorrelation require relatively larger samples as compared to populations with a high degree of autocorrelation.


# The population and the sample

```{r echo=FALSE, fig.margin=TRUE, fig.width=6, fig.height=3, fig.cap='The normal distribution, with a mean of 12% and standard deviation of 2%. Vertical lines mark the mean +/- 1 standard deviation.'}
n.x <- seq(5, 18, by=0.1)
n.y <- dnorm(seq(5, 18, by=0.1), mean=12, sd=2)
par(mar=c(4.5,0,0,0))
plot(n.x, n.y, type='l', axes=FALSE, xlab='Clay Content (%)', ylab='')
axis(side=1, labels = 5:18, at=5:18)
abline(v=c(10, 12, 14), lty=3)
```

For the sake of demonstration, lets pretend that we know some details about the population of possible clay contents associated with all A horizons, within a given soil series: the distribution shape is approximately normal, has a mean of 12% and a standard deviation of 2%. We can simulate these conditions by drawing a large number (1,000) of samples from the normal distribution. Having "access" to the clay content population is analogous to collecting all pixels (e.g. slope values) that overlap with a collection of map unit polygons. Next, draw a small sample (5% of the observations) from the population. This is analogous to the approach used by the MU summary reports: selecting pixels at a constant sampling density of approximately 1-5 points per acre.





