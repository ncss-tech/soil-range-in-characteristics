---
title: "Percentile Demonstration"
author: "D.E. Beaudette"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
    keep_md: no
---

```{r setup, echo=FALSE, results='hide', warning=FALSE}
# setup
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, cache = FALSE)
options(width=100, stringsAsFactors=FALSE, cache=FALSE)
```


```{r  echo=FALSE, results='hide', warning=FALSE}
library(soilDB)
library(scales)
library(Hmisc)
library(rgdal)
library(raster)
library(e1071)
library(sharpshootR)

# move this to sharpshootR eventually
percentileDemo <- function(x, labels.signif=3, pctile.color='RoyalBlue', mean.color='Orange', range.color='DarkRed', hist.breaks=30, boxp=FALSE, ...) {
  
  # convenient summary of vector x, can contain NA
  .summary <- function(x, pr=c(0.05, 0.1, 0.5, 0.9, 0.95)) {
    n <- length(na.omit(x))
    x.mean <- mean(x, na.rm=TRUE)
    x.skew <- skewness(x, na.rm=TRUE)
    x.sd <- sd(x, na.rm=TRUE)
    x.min <- min(x, na.rm=TRUE)
    x.max <- max(x, na.rm=TRUE)
    q <- t(hdquantile(x, probs = pr, na.rm=TRUE))
    dimnames(q)[[2]] <- paste0('P', pr * 100)
    d <- data.frame(mean=x.mean, sd=x.sd, skew=x.skew, min=x.min, q, max=x.max, n)
    return(d)
  }
  
  ## borrowed from MU reports
  # remove NA
  # re-scale to {0,1}
  # return x,y values
  .scaled.density <- function(x) {
    res <- stats::density(na.omit(x), cut=1)
    return(data.frame(x=res$x, y=scales::rescale(res$y)))
  }
  
  # prep values
  s <- .summary(x)
  x <- na.omit(x)
  
  # plot parameters
  y.base.pctiles <- -0.05
  y.base.normal <- -0.15
  y.base.range <- -0.25
  label.offset <- 0.04
  
  
  # density plot of the data, scaled to {0,1}
  d.x <- .scaled.density(x)
  
  # idealized normal over interval of the data
  x.seq <- seq(s$mean - 2.5 * s$sd, s$mean + 2.5 * s$sd, length.out = 100)
  ideal.normal <- cbind(x.seq, scales::rescale(dnorm(x.seq, mean=s$mean, sd=s$sd)))
  
  # histogram with "counts" re-scaled to {0,1}
  x.hist <- graphics::hist(x, plot=FALSE, breaks=hist.breaks)
  x.hist$counts <- scales::rescale(x.hist$counts)
  
  # setup plot
  plot(d.x, type='n', ylim=c(-0.3, 1.1), axes=FALSE, ...)
  abline(h=c(0, 1), col=grey(0.85))
  plot(x.hist, add=TRUE, col=grey(0.9), border=grey(0.85))
  lines(d.x, lwd=2)
  lines(ideal.normal)
  
  # add obs as vertical ticks
  # points(x, rep(1, times=length(x)), pch='|', cex=0.5, col=grey(0.75))
  
  
  ## boxplot for reference
  if(boxp) {
    y.base.range <- y.base.range - 0.02
    y.base.normal <- y.base.normal - 0.02
    y.base.pctiles <- y.base.pctiles - 0.02
    
    boxplot(x, at=-0.02, add=TRUE, axes=FALSE, horizontal=TRUE, boxwex=0.05, lwd=1, cex=0.5)
    # boxplot(rnorm(5000, mean=s$mean, sd=s$sd), at=1, add=TRUE, axes=FALSE, horizontal=TRUE, boxwex=0.05, col='grey')
  }
    
  
  ## percentiles
  # lower / upper pctile
  segments(x0=s$P10, x1=s$P90, y0=y.base.pctiles, y1=y.base.pctiles, col=pctile.color, lwd=2)
  segments(x0=c(s$P10, s$P90), x1=c(s$P10, s$P90), y0=y.base.pctiles, y1=1, lty=3, col=pctile.color)
  # median
  segments(x0=s$P50, x1=s$P50, y0=y.base.pctiles, y1=1, lty=3, col=pctile.color)
  points(s$P50, y.base.pctiles, cex=1.2, pch=22, bg=pctile.color)
  # add values
  text(x=c(s$P10, s$P50, s$P90), y=y.base.pctiles - label.offset, label=signif(c(s$P10, s$P50, s$P90), labels.signif), cex=0.65)
  
  ## idealized normal
  # +/- SD
  segments(x0=(-2 * s$sd) + s$mean, x1=(2 * s$sd) + s$mean, y0=y.base.normal, y1=y.base.normal, col=mean.color, lwd=2)
  segments(x0=(-2 * s$sd) + s$mean, x1=(-2 * s$sd) + s$mean, y0=y.base.normal, y1=1, lty=3, col=mean.color)
  segments(x0=(2 * s$sd) + s$mean, x1=(2 * s$sd) + s$mean, y0=y.base.normal, y1=1, lty=3, col=mean.color)
  # mean
  segments(x0=s$mean, x1=s$mean, y0=y.base.normal, y1=1, lty=3, col=mean.color)
  points(s$mean, y.base.normal, cex=1.25, pch=22, bg=mean.color)
  # add values
  text(x=c(((-2 * s$sd) + s$mean), s$mean, ((2 * s$sd) + s$mean)), y=y.base.normal - label.offset, label=signif(c(((-2 * s$sd) + s$mean), s$mean, ((2 * s$sd) + s$mean)), labels.signif), cex=0.65)
  
  ## min / max
  # +/- SD
  segments(x0=s$min, x1=s$max, y0=y.base.range, y1=y.base.range, col=range.color, lwd=2)
  segments(x0=s$min, x1=s$min, y0=y.base.range, y1=1, lty=3, col=range.color)
  segments(x0=s$max, x1=s$max, y0=y.base.range, y1=1, lty=3, col=range.color)
    # add values
  text(x=c(s$min, s$max), y=y.base.range - label.offset, label=signif(c(s$min, s$max), labels.signif-1), cex=0.65)
  
  # finish basic axix / box
  axis(side=1, at = pretty(x, n = 10))
  box()
  
  # combined legend
  legend('top', lwd=c(2,2,2,2,1, NA), lty=c(1,1,1,1,1, NA), col=c(pctile.color, mean.color, range.color, 'black', 'black'), legend = c('10th-50th-90th', 'Mean +/- 2SD', 'Min / Max', 'Source Dist', 'Ideal Normal', paste('Obs:', s$n)), horiz=TRUE, bty='n', cex=0.8)
  
  invisible(s)
}

# get a subset of a given variable from an SPC, by pattern matching against hz designation
subsetData <- function(SPC, hzname, hz_pat, var) {

  h <- horizons(SPC)
  idx <- grep(hz_pat, h[[hzname]])
  x <- h[[var]][idx]
  return(x)
}

```


```{r echo=FALSE, warning=FALSE}
reMakeData <- FALSE

if(reMakeData) {
  # get some sample data
  kssl <- fetchKSSL('miami')
  nasis <- fetchNASIS(rmHzErrors = FALSE)
  
  idx <- grep('loafercreek', nasis$taxonname, ignore.case = TRUE)
  loafercreek <- nasis[idx, ]
  
  idx <- grep('amador', nasis$taxonname, ignore.case = TRUE)
  amador <- nasis[idx, ]
  
  idx <- grep('Nedsgulch', nasis$taxonname, ignore.case = TRUE)
  nedsgulch <- nasis[idx, ]
  
  # best possible scenario: rasters are in memory
  r.elev <- readAll(raster('E:/gis_data/ca630/ca630_elev/hdr.adf'))
  r.slope <- readAll(raster('E:/gis_data/ca630/ca630_slope/hdr.adf'))
  r.MAP <- readAll(raster('E:/gis_data/prism/final_MAP_mm_800m.tif'))
  
  # load map unit polygons
  mu <-  readOGR(dsn='E:/gis_data/ca630/FG_CA630_OFFICIAL.gdb', layer='ca630_a', stringsAsFactors = FALSE)
  
  # add a unique polygon ID
  mu$pID <- seq(from=1, to=length(mu))
  
  # extract polygons for a single map unit ("MUSYM" attribute = "7089")
  # note that column names in your data may be different
  mu.5012 <- mu[which(mu$MUSYM == '5012'), ]
  
  # generate sampling points
  s <- constantDensitySampling(mu.5012, n.pts.per.ac=1, min.samples=1, polygon.id='pID')
  
  # extract raster data
  e.elev <- extract(r.elev, s)
  e.slope <- extract(r.slope, s)
  e.MAP <- extract(r.MAP, s)
  
  # save for later
  save(kssl, loafercreek, amador, nedsgulch, e.elev, e.slope, e.MAP, file='cached-data.rda')
} else {
  # get cached version
  load(file='cached-data.rda')
}

```

```{r echo=FALSE}
# example data for introduction
set.seed(101010)
x <- round(rnorm(11, mean=15, sd=4))
```

# Introduction
This document describes the *percentile* as a robust measure of central tendency and spread within a distribution of values. Examples are given for its application in the context of soil data summaries.

## Definition and Description of the Percentile
Within a set of data, the *n-th* [percentile](https://en.wikipedia.org/wiki/Percentile) describes the value below which *n%* of the data, when sorted, fall. For example, within the integer sequence spanning 0 to 100, *50* is the 50th percentile or median, *10* is the 10th percentile, and *90* is the 90th percentile.

Consider the following (hypothetical) field-described clay content from the A horizon of the same taxa:

`r x`

sorting,

`r sort(x)`

10th (`r quantile(x, probs=c(0.1))`), 50th (`r quantile(x, probs=c(0.5))`), and 90th (`r quantile(x, probs=c(0.9))`) percentiles.

## Why Percentiles?

* Percentiles require no distributional assumptions and are bound to the data from which they are computed. This means that percentiles can provide meaningful benchmarks for both normal and non-normal distributions, and, the limits will always fall within the min/max of the observed data.

* Direct interpretation; consider the 10th ($P_{10}$) and 90th ($P_{90}$) percentiles: "given the available data, we know that soil property $p < P_{10}$ 10% of the time, and, $p < P_{90}$ 90% of the time". This same statement can be framed using probabilities or proportions: "given the available data, soil property $p$ is within the range of $\{P_{10} - P_{90}\}$ 80% of the time".

* Percentiles are simple to calculate, requiring at least 3, better 10, and ideally $>$ 20 observations.

* The median is a robust estimator of [central tendency](https://en.wikipedia.org/wiki/Central_tendency).

* The lower and upper percentiles (e.g. 10th and 90th) a robust estimator of spread.


### Small Sample Sizes and Interpolation
Estimation of percentiles is based on ranking of the original data. Interpolation *between observed values* is required when there is a small number of samples. Consider the values `(1,3,5,6,7,9,9,10)`. Estimation of the 10th, 50th, and 90th percentiles results in `r round(Hmisc::hdquantile(c(1,3,5,6,7,9,9,10), probs = c(0.1, 0.5, 0.9)))` respectively. Since we are not typically interested in the estimated percentiles verbatim, the interpolated estimates are close enough. The [Harrel-Davis quantile estimator](https://rdrr.io/cran/Hmisc/man/hdquantile.html) is a robust method for estimating quantiles, no matter the sample size.



# Common Distribution Shapes

The following figures demonstrate the relationship between distribution shape, measures of central tendency (mean and 50th percentile, and measures of spread (mean +/1 2 standard deviations, and 10th / 90th percentiles). Within each figure is an *idealized* normal distribution that is based on the sample mean and standard deviation. The y-axis can be interpreted as the "relative proportion" of samples associated with a value on the x-axis. The thick, smooth lines represent an estimate of [density](https://en.wikipedia.org/wiki/Density_estimation), a continuous alternative to the [histogram](https://en.wikipedia.org/wiki/Histogram) (grey columns).

With a large enough sample size, the distribution of some soil properties can be approximated with the [normal or Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution). In this case, the mean and median are practically equal and the spread around the central tendency is balanced. Examples include lab measured clay content and pH from a suite of related samples (e.g. A horizons from a single soil series concept).

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(3,1,2,1))
set.seed(101010)
x <- rnorm(100, mean=15, sd=2)
res <- percentileDemo(x, xlab='', ylab='', main='Approximately Normal')
kable(res, digits = 1)
```

Various forms of the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) are typically more accurate approximations of soil properties. Log-normal distributions with a "short tail", or a low degree of asymmetry  ([skewness](https://en.wikipedia.org/wiki/Skewness)) around the central tendency, are common. Note the shift between mean and median, and the unequal distances to 10th and 90th percentiles. Examples include lab measured organic carbon and field measured rock fragment volume.
```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(3,1,2,1))
set.seed(101010)
x <- rlnorm(100, meanlog = log(15), sdlog = log(1.5))
res <- percentileDemo(x, xlab='', ylab='', main='Log-Normal: Short Tail')
kable(res, digits = 1)
```

Log-normal distributions with a "long tail", or a high degree of [skewness](https://en.wikipedia.org/wiki/Skewness) around the central tendency, are commonly encountered when summarizing GIS data sources such as elevation, slope, and curvature. Note that the mean +/- 2SD is now longer a meaningful representation of spread.
```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(3,1,2,1))
set.seed(101010)
x <- rlnorm(100, meanlog = log(15), sdlog = log(3))
res <- percentileDemo(x, xlab='', ylab='', main='Log-Normal: Long Tail')
kable(res, digits = 1)
```

In general, the further the departure from a normal distribution, the less meaningful mean and standard deviation are as metrics of central tendency and spread.


# Examples

## Lab Characterization Data

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Bt', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='KSSL: Miami, Bt Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'A', var = 'estimated_oc')
res <- percentileDemo(x, xlab='Organic Carbon (%)', main='KSSL: Miami, A Horizons')
kable(res, digits = 3)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(kssl, hzname = 'hzn_desgn', hz_pat = 'Bt', var = 'estimated_ph_h2o')
res <- percentileDemo(x, xlab='pH', main='KSSL: Miami, Bt Horizons', labels.signif = 2)
kable(res, digits = 1)
```


## Morpologic Data (NASIS)

### Pedons Correlated to Loafercreek

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'Bt', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Loafercreek, Bt Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'A', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='NASIS: Loafercreek, A Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(loafercreek, hzname = 'hzname', hz_pat = 'Bt', var = 'clay')
res <- percentileDemo(x, xlab='Percent Clay', main='NASIS: Loafercreek, Bt Horizons')
kable(res, digits = 1)
```

### Pedons Correlated to Nedsgulch

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'A', var = 'sand')
res <- percentileDemo(x, xlab='Percent Sand', main='NASIS: Nedsgulch, A Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'Bt', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Nedsgulch, Bt Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(nedsgulch, hzname = 'hzname', hz_pat = 'A', var = 'phfield')
res <- percentileDemo(x, xlab='pH', main='NASIS: Nedsgulch, A Horizons', labels.signif = 2)
kable(res, digits = 1)
```


### Pedons Correlated to Amador
Note that histogram and density estimates are not very helpful when sample size is small. Also, note that estimated percentiles are interpolated between actual observations.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(amador, hzname = 'hzname', hz_pat = 'A', var = 'clay')
res <- percentileDemo(x, xlab='Percent Sand', main='NASIS: Amador, A Horizons')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,2,1))
x <- subsetData(amador, hzname = 'hzname', hz_pat = 'A', var = 'total_frags_pct')
res <- percentileDemo(x, xlab='Rock Fragment Volume (%)', main='NASIS: Amador, A Horizons')
kable(res, digits = 1)
```

## GIS Data
Many sampled values leads to more reliable estimates of central tendency and spread.

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.elev, hist.breaks=90, xlab='Elevation (m)', main='CA630: Map Unit 5012')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.slope, hist.breaks=90, xlab='Slope (%)', main='CA630: Map Unit 5012')
kable(res, digits = 1)
```

```{r echo=FALSE, warning=FALSE, fig.width=9, fig.height=5.25}
par(mar=c(4.5,1,3,1))
res <- percentileDemo(e.MAP, hist.breaks=90, xlab='PRISM Mean Annual PPT (mm)', main='CA630: Map Unit 5012')
kable(res, digits = 1)
```







